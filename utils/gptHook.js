const OpenAI = require("openai");
const fs = require('fs');
const path = require('path');

require('dotenv').config();

const GPT_API_KEY = process.env.OPENAI_API_KEY;

if (!GPT_API_KEY) {
    console.error("‚ùå OPENAI_API_KEY kh√¥ng t√¨m th·∫•y trong file .env!");
    console.error("üîß Vui l√≤ng th√™m OPENAI_API_KEY v√†o file .env");
}

// C·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho t·ª´ng lo·∫°i request
const DEFAULT_CONFIGS = {
    creative: {
        temperature: 1.2,
        max_tokens: 1000,
        presence_penalty: 0.6,
        frequency_penalty: 0.3
    },
    analytical: {
        temperature: 0.7,
        max_tokens: 1500,
        presence_penalty: 0.3,
        frequency_penalty: 0.1
    },
    conversational: {
        temperature: 0.9,
        max_tokens: 800,
        presence_penalty: 0.6,
        frequency_penalty: 0.3
    },
    educational: {
        temperature: 0.6,
        max_tokens: 1200,
        presence_penalty: 0.2,
        frequency_penalty: 0.1
    },
    fun: {
        temperature: 1.1,
        max_tokens: 600,
        presence_penalty: 0.8,
        frequency_penalty: 0.5
    }
};

/**
 * Hook API GPT t·ªïng h·ª£p
 * @param {Object} options - C·∫•u h√¨nh request
 * @param {string} options.prompt - Prompt ch√≠nh
 * @param {string} options.systemPrompt - System prompt (optional)
 * @param {string} options.type - Lo·∫°i request: creative, analytical, conversational, educational, fun
 * @param {Object} options.config - C·∫•u h√¨nh t√πy ch·ªânh (optional)
 * @param {Array} options.usedContent - N·ªôi dung ƒë√£ s·ª≠ d·ª•ng ƒë·ªÉ tr√°nh l·∫∑p (optional)
 * @param {string} options.context - Context b·ªï sung (optional)
 * @returns {Promise<string>} Response t·ª´ GPT
 */
const useGPT = async (options) => {
    const {
        prompt,
        systemPrompt = "",
        type = "conversational",
        config = {},
        usedContent = [],
        context = ""
    } = options;

    if (!prompt) {
        throw new Error("Prompt is required");
    }

    if (!GPT_API_KEY) {
        throw new Error("OpenAI API key not configured in .env file");
    }

    // Merge c·∫•u h√¨nh
    const finalConfig = {
        ...DEFAULT_CONFIGS[type] || DEFAULT_CONFIGS.conversational,
        ...config
    };

    // T·∫°o system prompt ƒë·∫ßy ƒë·ªß
    const fullSystemPrompt = buildSystemPrompt(systemPrompt, type, usedContent, context);

    try {
        return await callGPT(prompt, fullSystemPrompt, finalConfig);
    } catch (error) {
        console.error(`Error with GPT:`, error);
        throw error;
    }
};

/**
 * T·∫°o system prompt d·ª±a tr√™n type v√† context
 */
const buildSystemPrompt = (customPrompt, type, usedContent, context) => {
    let basePrompt = "";

    switch (type) {
        case "creative":
            basePrompt = "B·∫°n l√† m·ªôt AI s√°ng t·∫°o, c√≥ kh·∫£ nƒÉng t·∫°o ra n·ªôi dung ƒë·ªôc ƒë√°o v√† th√∫ v·ªã.";
            break;
        case "analytical":
            basePrompt = "B·∫°n l√† m·ªôt AI ph√¢n t√≠ch, chuy√™n v·ªÅ logic v√† suy lu·∫≠n.";
            break;
        case "conversational":
            basePrompt = "B·∫°n l√† m·ªôt AI th√¢n thi·ªán, n√≥i chuy·ªán t·ª± nhi√™n nh∆∞ con ng∆∞·ªùi.";
            break;
        case "educational":
            basePrompt = "B·∫°n l√† m·ªôt gi√°o vi√™n AI, gi·ªèi gi·∫£i th√≠ch kh√°i ni·ªám ph·ª©c t·∫°p m·ªôt c√°ch ƒë∆°n gi·∫£n.";
            break;
        case "fun":
            basePrompt = "B·∫°n l√† m·ªôt AI vui v·∫ª, h√†i h∆∞·ªõc v√† gi·∫£i tr√≠.";
            break;
    }

    let fullPrompt = basePrompt;

    if (customPrompt) {
        fullPrompt += "\n\n" + customPrompt;
    }

    if (context) {
        fullPrompt += "\n\nContext: " + context;
    }

    if (usedContent.length > 0) {
        fullPrompt += "\n\nƒê√£ s·ª≠ d·ª•ng tr∆∞·ªõc ƒë√≥ (TR√ÅNH L·∫∂P L·∫†I):\n" + usedContent.join("\n");
    }

    fullPrompt += "\n\nH√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n v√† ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh.";

    return fullPrompt;
};

/**
 * G·ªçi GPT API
 */
const callGPT = async (prompt, systemPrompt, config) => {
    const openai = new OpenAI({
        apiKey: GPT_API_KEY
    });

    const result = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
            {
                role: "system",
                content: systemPrompt
            },
            {
                role: "user",
                content: prompt
            }
        ],
        temperature: config.temperature,
        max_tokens: config.max_tokens,
        presence_penalty: config.presence_penalty,
        frequency_penalty: config.frequency_penalty
    });

    return result.choices[0].message.content.trim();
};

/**
 * Hook ƒë·∫∑c bi·ªát cho tr√°nh l·∫∑p l·∫°i n·ªôi dung
 */
const useGPTWithHistory = async (options) => {
    const { historyFile, maxHistory = 100 } = options;

    let history = [];
    if (historyFile && fs.existsSync(historyFile)) {
        try {
            history = JSON.parse(fs.readFileSync(historyFile, 'utf8'));
        } catch (error) {
            console.warn("Could not load history:", error);
        }
    }

    // Th√™m history v√†o usedContent
    const usedContent = history.map(item => item.content || item.response || item).slice(-10);
    
    const response = await useGPT({
        ...options,
        usedContent: [...(options.usedContent || []), ...usedContent]
    });

    // L∆∞u response v√†o history
    if (historyFile) {
        history.push({
            content: response,
            timestamp: Date.now()
        });

        // Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng history
        if (history.length > maxHistory) {
            history = history.slice(-maxHistory);
        }

        try {
            const dir = path.dirname(historyFile);
            if (!fs.existsSync(dir)) {
                fs.mkdirSync(dir, { recursive: true });
            }
            fs.writeFileSync(historyFile, JSON.stringify(history, null, 2));
        } catch (error) {
            console.warn("Could not save history:", error);
        }
    }

    return response;
};

/**
 * Hook cho streaming response (t∆∞∆°ng lai)
 */
const useGPTStream = async (options, onChunk) => {
    // TODO: Implement streaming
    const response = await useGPT(options);
    if (onChunk) {
        onChunk(response);
    }
    return response;
};

/**
 * Utility function ƒë·ªÉ format response
 */
const formatResponse = (response, format = "plain") => {
    switch (format) {
        case "markdown":
            return response; // Already in markdown
        case "plain":
            return response.replace(/[*_`#]/g, "");
        case "html":
            return response
                .replace(/\*\*(.*?)\*\*/g, "<strong>$1</strong>")
                .replace(/\*(.*?)\*/g, "<em>$1</em>")
                .replace(/`(.*?)`/g, "<code>$1</code>");
        default:
            return response;
    }
};

/**
 * Batch processing multiple prompts
 */
const useGPTBatch = async (prompts, options = {}) => {
    const results = [];
    
    for (const prompt of prompts) {
        try {
            const response = await useGPT({
                ...options,
                prompt
            });
            results.push({ success: true, response });
            
            // Delay between requests
            await new Promise(resolve => setTimeout(resolve, 1000));
        } catch (error) {
            results.push({ success: false, error: error.message });
        }
    }
    
    return results;
};

module.exports = {
    useGPT,
    useGPTWithHistory,
    useGPTStream,
    useGPTBatch,
    formatResponse,
    DEFAULT_CONFIGS
};